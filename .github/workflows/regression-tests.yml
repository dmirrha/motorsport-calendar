name: Regression Tests

on:
  push:
    branches: [ main, develop, 'feature/*' ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:
    inputs:
      log-level:
        description: 'Log level (debug, info, warning, error, critical)'
        required: false
        default: 'info'

jobs:
  test:
    name: Run Regression Tests
    runs-on: ubuntu-latest
    
    services:
      # Adicione aqui serviços adicionais necessários para os testes
      # Por exemplo, um banco de dados em memória
      redis:
        image: redis
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        # Instala as dependências principais
        pip install -r requirements.txt
        # Instala explicitamente as dependências de teste
        pip install pytest pytest-cov pytest-html pytest-xdist
        # Instala o pacote em modo desenvolvimento para garantir que todos os módulos estejam disponíveis
        pip install -e .

    - name: Create required directories and config
      run: |
        # Create required directories
        mkdir -p test_results
        mkdir -p logs
        mkdir -p output
        mkdir -p config
        
        # Create a complete config file with all required sections
        echo "Creating config/config.json with all required sections..."
        cat > config/config.json << 'EOL'
        {
          "sources": [
            {
              "name": "tomada_tempo",
              "enabled": true,
              "base_url": "https://www.tomadatempo.com.br",
              "endpoints": {
                "programacao": "/programacao",
                "evento": "/evento",
                "categorias": "/categorias"
              },
              "timeout": 5,
              "retry_attempts": 2,
              "headers": {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
              }
            }
          ],
          "output": {
            "directory": "./test_output",
            "filename_format": "events_{date}.ics",
            "date_format": "%Y%m%d",
            "overwrite_existing": true
          },
          "logging": {
            "level": "INFO",
            "file": "test_logs/test.log",
            "max_size_mb": 10,
            "backup_count": 5,
            "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
          },
          "retry": {
            "max_attempts": 3,
            "backoff_factor": 1,
            "status_forcelist": [500, 502, 503, 504]
          },
          "ui": {
            "enabled": true,
            "theme": "dark",
            "refresh_interval_seconds": 300,
            "show_progress_bars": true
          },
          "calendar": {
            "timezone": "UTC",
            "default_duration_minutes": 120,
            "reminders": [
              {
                "minutes": 60,
                "method": "popup"
              },
              {
                "minutes": 15,
                "method": "popup"
              }
            ]
          },
          "filters": {
            "include_categories": ["formula1", "formula2", "formula3", "nascar", "wec", "indycar", "motogp"],
            "exclude_categories": [],
            "min_duration_minutes": 30,
            "max_duration_hours": 6
          },
          "event_processing": {
            "timezone_handling": "convert_to_utc",
            "duplicate_detection": {
              "enabled": true,
              "similarity_threshold": 0.8,
              "time_window_minutes": 30
            },
            "date_parsing": {
              "prefer_locale_dates": true,
              "fallback_timezone": "America/Sao_Paulo"
            }
          },
          "general": {
            "timezone": "UTC",
            "language": "en-US",
            "log_level": "INFO",
            "output_directory": "./test_output"
          }
        }
        EOL
        
        # Verify the file was created
        if [ -f "config/config.json" ]; then
          echo "Successfully created config/config.json"
          echo "Config file content:"
          cat config/config.json
        else
          echo "Failed to create config/config.json"
          exit 1
        fi

    - name: Run tests
      run: |
        # Ensure test results directory exists with proper permissions
        mkdir -p test_results
        chmod -R 777 test_results
        
        # Show current directory structure for debugging
        echo "Current directory structure:"
        pwd
        ls -la
        
        # Install package in development mode to ensure imports work
        pip install -e .
        
        # Run pytest with full verbosity and show traceback for debugging
        # Generate coverage.xml in the root directory for Codecov
        python -m pytest -v --tb=short \
          --junitxml=test_results/junit.xml \
          --cov=src \
          --cov=sources \
          --cov-report=xml:coverage.xml \
          --cov-report=term-missing \
          --html=test_results/report.html \
          --self-contained-html
        
        # Verify test results were generated
        echo "Root directory contents:"
        ls -la
        
        echo "Test results directory contents:"
        ls -la test_results/
        
        # Verify if coverage.xml exists and has content
        if [ -f "coverage.xml" ]; then
          echo "coverage.xml exists and has $(wc -l < coverage.xml) lines"
          # Copy coverage.xml to test_results for archiving
          cp coverage.xml test_results/
        else
          echo "ERROR: coverage.xml not found in root directory"
          exit 1
        fi
        
        # Verify if junit.xml exists and has content
        if [ -f "test_results/junit.xml" ]; then
          echo "junit.xml exists and has $(wc -l < test_results/junit.xml) lines"
        else
          echo "ERROR: junit.xml not found in test_results/"
          exit 1
        fi
        
      env:
        PYTHONPATH: ${{ github.workspace }}
        TEST_ENV: github
        PYTHONUNBUFFERED: 1

    - name: Upload coverage to Codecov
      if: always()
      uses: codecov/codecov-action@v3
      with:
        token: ${{ secrets.CODECOV_TOKEN }}
        fail_ci_if_error: true
        verbose: true

    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: test-results
        path: |
          test_results/**/*
          !test_results/.gitkeep
        retention-days: 7
        compression-level: 0  # Desativa a compressão para acelerar o upload

    - name: Check test results
      if: always()
      run: |
        # Mostra o conteúdo do diretório de resultados para depuração
        echo "Conteúdo do diretório de trabalho:"
        pwd
        ls -la
        
        # Verifica se o diretório de resultados existe
        if [ ! -d "test_results" ]; then
          echo "::error::Diretório test_results não encontrado."
          exit 1
        fi
        
        # Verifica se o arquivo de resultados JUnit existe
        if [ ! -f "test_results/junit.xml" ]; then
          echo "::error::Arquivo de resultados JUnit (junit.xml) não encontrado em test_results/"
          echo "Conteúdo de test_results/:"
          ls -la test_results/
          exit 1
        fi
        
        # Verifica se houve falhas nos testes
        echo "Verificando falhas em test_results/junit.xml..."
        if grep -q 'failures="[1-9]' test_results/junit.xml; then
          echo "::error::Alguns testes falharam. Verifique os logs para mais detalhes."
          # Mostra um resumo das falhas, se disponível
          if command -v xmllint &> /dev/null; then
            echo "Resumo das falhas:"
            xmllint --xpath '//testcase[failure]' test_results/junit.xml || true
          fi
          exit 1
        elif grep -q 'errors="[1-9]' test_results/junit.xml; then
          echo "::error::Erros encontrados durante a execução dos testes. Verifique os logs para mais detalhes."
          # Mostra um resumo dos erros, se disponível
          if command -v xmllint &> /dev/null; then
            echo "Resumo dos erros:"
            xmllint --xpath '//testcase[error]' test_results/junit.xml || true
          fi
          exit 1
        else
          echo "Todos os testes passaram com sucesso!"
        fi
        
        # Verifica se o relatório de cobertura foi gerado
        if [ ! -f "coverage.xml" ] && [ ! -f "test_results/coverage.xml" ]; then
          echo "::warning::Relatório de cobertura (coverage.xml) não encontrado no diretório raiz nem em test_results/"
        else
          echo "Relatório de cobertura gerado com sucesso."
        fi
