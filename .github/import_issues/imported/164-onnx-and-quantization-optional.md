# F3 (opcional): ONNX e quantizaÃ§Ã£o para aceleraÃ§Ã£o local

## ğŸ“ DescriÃ§Ã£o
Converter o modelo para ONNX e habilitar execuÃ§Ã£o opcional via `onnxruntime` (providers auto: CPU/MPS/GPU). Avaliar ganhos de latÃªncia/throughput e qualidade.

## ğŸ” Contexto
- NÃ£o deve ser padrÃ£o; habilitar por `ai.onnx.enabled=true`.
- Providers configurÃ¡veis (`ai.onnx.providers`).

## ğŸ¯ Comportamento Esperado
- Melhorar latÃªncia sem perder qualidade acima da tolerÃ¢ncia definida no plano.
- Manter fallback para execuÃ§Ã£o padrÃ£o.

## ğŸ› ï¸ Passos
1. Pipeline de export PTâ†’ONNX (ou direto do Transformers) e testes de consistÃªncia.
2. IntegraÃ§Ã£o no `embeddings_service` para usar ORT quando enabled.
3. Benchmarks comparativos e documentaÃ§Ã£o de setup.

## ğŸ“‹ CritÃ©rios de AceitaÃ§Ã£o
- [ ] ExecuÃ§Ã£o ONNX habilitÃ¡vel por config; fallback seguro.
- [ ] Benchmarks mostram ganhos claros ou decisÃ£o documentada.
- [ ] Sem regressÃ£o de qualidade acima do limite.

## ğŸ“Š Impacto
MÃ©dio â€” otimizaÃ§Ã£o de performance opcional.

## ğŸ”— ReferÃªncias
- `docs/architecture/ai_implementation_plan.md`
- `requirements.txt`
- `src/ai/embeddings_service.py`
