# F0: AvaliaÃ§Ã£o e benchmarks (baseline vs IA)

## ğŸ“ DescriÃ§Ã£o
Criar scripts e cenÃ¡rios de avaliaÃ§Ã£o para comparar baseline (heurÃ­stico/fuzzy) vs IA (semÃ¢ntico) em categorizaÃ§Ã£o e deduplicaÃ§Ã£o, com coleta de mÃ©tricas e reproducibilidade.

## ğŸ” Contexto
- Pasta sugerida para relatÃ³rios: `docs/tests/audit/` ou `docs/tests/scenarios/`.
- MÃ©tricas: precisÃ£o, cobertura, latÃªncia por lote, cache hit rate.

## ğŸ¯ Comportamento Esperado
- Facilitar execuÃ§Ã£o local com seeds fixos e dataset de exemplo.
- Exportar relatÃ³rios Markdown/CSV simples.

## ğŸ› ï¸ Passos
1. Preparar dataset sintÃ©tico/exemplar e seeds.
2. Rodar baseline vs IA (cat e dedup) e cronometrar.
3. Gerar relatÃ³rios comparativos com grÃ¡ficos simples (opcional).
4. Documentar como rodar e interpretar.

## ğŸ“‹ CritÃ©rios de AceitaÃ§Ã£o
- [ ] Scripts documentados e reprodutÃ­veis.
- [ ] MÃ©tricas salvas em arquivos versionÃ¡veis.
- [ ] Sem dependÃªncia de nuvem.

## ğŸ“Š Impacto
MÃ©dio â€” garante visibilidade de qualidade/performance e reduz risco.

## ğŸ”— Relacionamento
- EPIC: #157

## ğŸ”— ReferÃªncias
- `docs/architecture/ai_implementation_plan.md`
- `requirements.txt`
- CÃ³digo: `src/category_detector.py`, `src/event_processor.py`
