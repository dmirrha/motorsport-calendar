# F1: ServiÃ§o local de embeddings (cache+batch)

## ğŸ“ DescriÃ§Ã£o
Implementar um serviÃ§o local de embeddings para suporte semÃ¢ntico (multilÃ­ngue) usando modelo `paraphrase-multilingual-MiniLM-L12-v2`, com batching e cache (memÃ³ria/disco), executando offline (CPU por padrÃ£o; GPU/MPS se disponÃ­vel).

## ğŸ” Contexto
- IntegraÃ§Ãµes futuras: categorizaÃ§Ã£o (issue #147) e deduplicaÃ§Ã£o (issue #148).
- PadrÃ£o opt-in: `ai.enabled=false`.
- Arquivos alvo: `src/ai/embeddings_service.py`, `src/ai/cache.py` (novos), integraÃ§Ã£o via `src/category_detector.py` e `src/event_processor.py`.

## ğŸ¯ Comportamento Esperado
- ServiÃ§o carrega modelo localmente; sem chamadas de rede.
- Cache hit-rate reportÃ¡vel; batching configurÃ¡vel.
- Fallback seguro: se `ai.enabled=false`, nÃ£o deve impactar pipeline.

## ğŸ› ï¸ Passos
1. Criar `src/ai/embeddings_service.py` com interface simples: `embed_texts(list[str]) -> list[vec]`.
2. Implementar cache LRU em memÃ³ria e cache persistente opcional em disco (`ai.cache_dir`).
3. Suportar `ai.device=auto` (CPU/MPS/GPU) e `ai.batch_size`.
4. Logging e mÃ©tricas: tempo por lote, cache hits/misses.
5. Testes unitÃ¡rios determinÃ­sticos (seed) com lotes pequenos.

## ğŸ“‹ CritÃ©rios de AceitaÃ§Ã£o
- [ ] Rodar 100% offline; sem dependÃªncias de rede em runtime.
- [ ] Batching e cache habilitados e configurÃ¡veis.
- [ ] Testes unitÃ¡rios cobrindo casos de cache, batch e device fallback.
- [ ] DocumentaÃ§Ã£o de setup e troubleshooting local.

## ğŸ“Š Impacto
MÃ©dio/Alto â€” base para recursos semÃ¢nticos com controle de performance local.

## ğŸ”— ReferÃªncias
- Plano: `docs/architecture/ai_implementation_plan.md`
- CÃ³digo: `src/category_detector.py`, `src/event_processor.py`
- Config: `src/utils/config_validator.py`, `config/config.example.json`
- Requisitos: `requirements.txt`
